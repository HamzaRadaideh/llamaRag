# Abstract:
This project explores integrating Retrieval-Augmented Generation (RAG) and vector search to enhance large language models (LLMs) like GPT-4 and LLaMA 3. These technologies address limitations in traditional LLMs, such as outdated information and lack of source transparency. We demonstrate these concepts through a Python-based code analysis project, utilizing tools like LLaMA 3 and Ollama, deployed via scalable infrastructure.

# Fine-Tuning:
The process of taking a pre-trained model and adapting it to a specific task by training it further on a smaller, task-specific dataset. This technique allows the model to achieve better performance on the target task while leveraging the knowledge it has already acquired.

# RAG (Retrieval Augmented Generation):
RAG connects LLMs to real-time data stores, allowing retrieval of specific, up-to-date data to generate more accurate and trustworthy responses. This approach mitigates the limitations of traditional LLMs, which rely on static training data and lack source transparency.

* Problems with Current LLMs:
Traditional LLMs, such as GPT-4, are constrained by static training data, leading to outdated responses and lack of source citation, which hampers verification and accuracy.
* How RAG Works:
RAG involves connecting an LLM to a real-time data store. Queries are processed by retrieving relevant, current data, which is then integrated into the LLM's response generation, ensuring answers are accurate and verifiable.


<br>
<br>
<br>


# Graduation Project 

# COMPS Team ðŸ’»

# Github team members:

- Hamza Radaideh: https://github.com/HamzaRadaideh
- Ahmad Elwan: https://github.com/AhmadElwan
- Mohammad Subeihi: https://github.com/mosubehi
- Khaled Samara: https://github.com/Khaled3303

